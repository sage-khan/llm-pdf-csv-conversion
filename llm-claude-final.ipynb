{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "V3phY3Z3aAGs",
    "outputId": "ec2ef2da-bcc8-4abe-a00f-52ee988a98a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anthropic\n",
      "  Downloading anthropic-0.34.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from anthropic)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic)\n",
      "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.9.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.19.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.8)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->anthropic)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.23.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.24.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)\n",
      "Downloading anthropic-0.34.2-py3-none-any.whl (891 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.9/891.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, anthropic\n",
      "Successfully installed anthropic-0.34.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
      "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-4.3.1\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install anthropic\n",
    "!pip install pypdf\n",
    "!pip install pandas\n",
    "!pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRLQULkDnaIY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from anthropic import Anthropic\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "6NchlQpQKZp9",
    "outputId": "c125acf9-212b-459c-a581-d2a17a8afaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved split PDF: ./a12/split_pdf/a12-0001.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0002.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0003.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0004.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0005.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0006.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0007.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0008.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0009.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0010.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0011.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0012.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0013.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0014.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0015.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0016.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0017.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0018.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0019.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0020.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0021.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0022.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0023.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0024.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0025.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0026.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0027.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0028.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0029.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0030.pdf\n",
      "Saved split PDF: ./a12/split_pdf/a12-0031.pdf\n",
      "Merged CSV saved: ./a12/converted_csv/a12_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-3f4ffb78d914>:138: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([merged_df, df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from anthropic import Client\n",
    "\n",
    "# Setup the API client with your API key\n",
    "os.environ['ANTHROPIC_API_KEY'] = \"your_antropic_api_key\"\n",
    "anthropic_client = Client(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "MODEL_NAME = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "# Define the path for the error log file\n",
    "error_log_path = \"./error_logs.txt\"\n",
    "\n",
    "# Function to log errors to the error log file\n",
    "def log_error(message):\n",
    "    with open(error_log_path, 'a') as log_file:\n",
    "        log_file.write(message + '\\n')\n",
    "\n",
    "# Function to get a completion from the Claude model\n",
    "def get_completion(client, prompt):\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=MODEL_NAME,\n",
    "            max_tokens=8192,\n",
    "            messages=[{\n",
    "                \"role\": 'user', \"content\": prompt\n",
    "            }]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error fetching completion from API: {e}\"\n",
    "        print(error_message)\n",
    "        log_error(error_message)\n",
    "        return None\n",
    "\n",
    "# Function to extract JSON content from a response string\n",
    "def extract_json_from_response(response):\n",
    "    try:\n",
    "        # Find the first occurrence of '[' and ']', and extract the JSON content\n",
    "        start_index = response.find('[')\n",
    "        end_index = response.find(']', start_index) + 1  # Include the closing bracket\n",
    "\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            json_content = response[start_index:end_index]\n",
    "            try:\n",
    "                # Parse the JSON content\n",
    "                json_data = json.loads(json_content)\n",
    "                return json_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                error_message = f\"Error parsing JSON content: {e}\"\n",
    "                print(error_message)\n",
    "                log_error(error_message)\n",
    "                return None\n",
    "        else:\n",
    "            error_message = \"No valid JSON found in the response.\"\n",
    "            print(error_message)\n",
    "            log_error(error_message)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        error_message = f\"Unexpected error while extracting JSON: {e}\"\n",
    "        print(error_message)\n",
    "        log_error(error_message)\n",
    "        return None\n",
    "\n",
    "# Function to split PDF into individual pages\n",
    "def split_pdf(pdf_path, split_folder):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            writer = PdfWriter()\n",
    "            writer.add_page(page)\n",
    "\n",
    "            # Update naming convention to pdfname-0001.pdf, pdfname-0002.pdf, etc.\n",
    "            split_pdf_path = os.path.join(split_folder, f'{pdf_name}-{i+1:04d}.pdf')\n",
    "            with open(split_pdf_path, 'wb') as output_pdf:\n",
    "                writer.write(output_pdf)\n",
    "\n",
    "            print(f\"Saved split PDF: {split_pdf_path}\")\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error splitting PDF {pdf_path}: {e}\"\n",
    "        print(error_message)\n",
    "        log_error(error_message)\n",
    "\n",
    "# Function to extract JSON data from a single PDF page\n",
    "def extract_json_from_page(pdf_page_path, output_folder):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_page_path)\n",
    "        text = ''.join(page.extract_text() for page in reader.pages)\n",
    "\n",
    "        # Define the prompt for Claude\n",
    "        prompt = f\"\"\"You are a seasoned data scientist at a fortune 500 company, Here is a document: <doc>{text}</doc>. Detect tables in the documents. Give all the transactions made in the document without making any assumptions and don't give samples. Only give everything that is given in the document. Convert to a completed JSON format with containing all of its required details\"\"\"\n",
    "\n",
    "        # Get JSON completion from Claude\n",
    "        completion = get_completion(anthropic_client, prompt)\n",
    "\n",
    "        if completion is None:\n",
    "            log_error(f\"Skipping file {pdf_page_path} due to API error.\")\n",
    "            return None\n",
    "\n",
    "        # Extract and parse the JSON portion from the completion\n",
    "        json_data = extract_json_from_response(completion)\n",
    "\n",
    "        if json_data is None:\n",
    "            log_error(f\"Skipping file {pdf_page_path} due to JSON extraction issues.\")\n",
    "            return None\n",
    "\n",
    "        # Save JSON file with the same name as the PDF page (e.g., pdfname-0001.json)\n",
    "        page_name = os.path.splitext(os.path.basename(pdf_page_path))[0]\n",
    "        json_path = os.path.join(output_folder, f'{page_name}.json')\n",
    "\n",
    "        # Save JSON to the specified output folder\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(json_data, f, indent=4)  # Save in a readable format\n",
    "\n",
    "        return json_path\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing PDF page {pdf_page_path}: {e}\"\n",
    "        print(error_message)\n",
    "        log_error(error_message)\n",
    "        return None\n",
    "\n",
    "# Function to merge multiple JSON files into a single DataFrame and save as CSV\n",
    "def merge_json_to_csv(json_folder, csv_output_folder, csv_filename):\n",
    "    try:\n",
    "        json_files = sorted([f for f in os.listdir(json_folder) if f.endswith('.json')])\n",
    "\n",
    "        merged_df = pd.DataFrame()\n",
    "\n",
    "        for json_file in json_files:\n",
    "            json_path = os.path.join(json_folder, json_file)\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.json_normalize(data)\n",
    "            merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "\n",
    "        # Generate CSV file path\n",
    "        csv_path = os.path.join(csv_output_folder, csv_filename)\n",
    "\n",
    "        # Save merged DataFrame as CSV\n",
    "        merged_df.to_csv(csv_path, index=False)\n",
    "        print(f\"Merged CSV saved: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error merging JSON to CSV: {e}\"\n",
    "        print(error_message)\n",
    "        log_error(error_message)\n",
    "\n",
    "# Main function to process PDFs\n",
    "def process_pdfs(pdf_folder, output_folder):\n",
    "    # List all PDF files in the folder\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "\n",
    "        # Create output folders for each PDF\n",
    "        pdf_name = os.path.splitext(pdf_file)[0]\n",
    "        pdf_output_folder = os.path.join(output_folder, pdf_name)\n",
    "        split_folder = os.path.join(pdf_output_folder, 'split_pdf')\n",
    "        converted_json_folder = os.path.join(pdf_output_folder, 'converted_json')\n",
    "        converted_csv_folder = os.path.join(pdf_output_folder, 'converted_csv')\n",
    "        os.makedirs(split_folder, exist_ok=True)\n",
    "        os.makedirs(converted_json_folder, exist_ok=True)\n",
    "        os.makedirs(converted_csv_folder, exist_ok=True)\n",
    "\n",
    "        # Split PDF into individual pages\n",
    "        split_pdf(pdf_path, split_folder)\n",
    "\n",
    "        # Process each split page to JSON\n",
    "        for split_pdf_file in sorted(os.listdir(split_folder)):\n",
    "            split_pdf_path = os.path.join(split_folder, split_pdf_file)\n",
    "            extract_json_from_page(split_pdf_path, converted_json_folder)\n",
    "\n",
    "        # Merge JSON files and convert to CSV\n",
    "        merge_json_to_csv(converted_json_folder, converted_csv_folder, f'{pdf_name}_merged.csv')\n",
    "\n",
    "# Specify the PDF folder and desired output location\n",
    "pdf_folder = \"./PDFs\"\n",
    "output_folder = \"./\"\n",
    "\n",
    "# Process the PDFs\n",
    "process_pdfs(pdf_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYH-T8ZmKbC9"
   },
   "source": [
    "#Old Code For refrence DON'T RUN!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3ecLlUcZUOC",
    "outputId": "5f7551cb-00e3-4e20-f7b2-aa4c67c4e7db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing JSON content: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "from anthropic import Client\n",
    "\n",
    "# Setup the API client with your API key\n",
    "os.environ['ANTHROPIC_API_KEY'] = \"your_anthropic_api_key\"\n",
    "anthropic_client = Client(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "MODEL_NAME = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "# Define the path for the error log file\n",
    "error_log_path = \"./error_logs.txt\"\n",
    "\n",
    "# Function to log errors to the error log file\n",
    "def log_error(message):\n",
    "    with open(error_log_path, 'a') as log_file:\n",
    "        log_file.write(message + '\\n')\n",
    "\n",
    "# Function to get a completion from the Claude model\n",
    "def get_completion(client, prompt):\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=MODEL_NAME,\n",
    "            max_tokens=8192,\n",
    "            messages=[{\n",
    "                \"role\": 'user', \"content\": prompt\n",
    "            }]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error fetching completion from API: {e}\"\n",
    "        print(error_message)\n",
    "        log_error(error_message)\n",
    "        return None\n",
    "\n",
    "# Function to extract JSON content from a response string\n",
    "def extract_json_from_response(response):\n",
    "    try:\n",
    "        # Find the first occurrence of '[' and ']', and extract the JSON content\n",
    "        start_index = response.find('[')\n",
    "        end_index = response.find(']', start_index) + 1  # Include the closing bracket\n",
    "\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            json_content = response[start_index:end_index]\n",
    "            try:\n",
    "                # Parse the JSON content\n",
    "                json_data = json.loads(json_content)\n",
    "                return json_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                error_message = f\"Error parsing JSON content: {e}\"\n",
    "                print(error_message)\n",
    "                log_error(error_message)\n",
    "                return None\n",
    "        else:\n",
    "            error_message = \"No valid JSON found in the response.\"\n",
    "            print(error_message)\n",
    "            log_error(error_message)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        error_message = f\"Unexpected error while extracting JSON: {e}\"\n",
    "        print(error_message)\n",
    "        log_error(error_message)\n",
    "        return None\n",
    "\n",
    "# Function to extract JSON data from PDF\n",
    "def extract_json_from_pdf(pdf_path, output_folder):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = ''.join(page.extract_text() for page in reader.pages)\n",
    "\n",
    "        # Define the prompt for Claude\n",
    "        prompt = f\"\"\"You are a seasoned data scientist at a fortune 500 company, Here is a document: <doc>{text}</doc>. Detect tables in the documents. Give all the transactions made in the document without making any assumptions and don't give samples. Only give everything that is given in the document. Convert to a completed JSON format with containing all of its required details\"\"\"\n",
    "\n",
    "        # Get JSON completion from Claude\n",
    "        completion = get_completion(anthropic_client, prompt)\n",
    "\n",
    "        if completion is None:\n",
    "            log_error(f\"Skipping file {pdf_path} due to API error.\")\n",
    "            return None\n",
    "\n",
    "        # Extract and parse the JSON portion from the completion\n",
    "        json_data = extract_json_from_response(completion)\n",
    "\n",
    "        if json_data is None:\n",
    "            log_error(f\"Skipping file {pdf_path} due to JSON extraction issues.\")\n",
    "            return None\n",
    "\n",
    "        # Generate JSON file path using the PDF name\n",
    "        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        json_path = os.path.join(output_folder, 'extracted_json', f'{pdf_name}.json')\n",
    "\n",
    "        # Save JSON to the specified output folder\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(json_data, f, indent=4)  # Save in a readable format\n",
    "\n",
    "        return json_path\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing PDF {pdf_path}: {e}\"\n",
    "        print(error_message)\n",
    "        log_error(error_message)\n",
    "        return None\n",
    "\n",
    "# Function to convert JSON to CSV\n",
    "def convert_json_to_csv(json_path, csv_output_folder):\n",
    "    try:\n",
    "        if not json_path:\n",
    "            log_error(\"Skipping CSV conversion due to JSON parsing error.\")\n",
    "            return\n",
    "\n",
    "        # Load JSON data\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Convert JSON to DataFrame\n",
    "        df = pd.json_normalize(data)\n",
    "\n",
    "        # Generate CSV file path using the JSON file name\n",
    "        csv_filename = os.path.splitext(os.path.basename(json_path))[0] + '.csv'\n",
    "        csv_path = os.path.join(csv_output_folder, 'converted_csv', csv_filename)\n",
    "\n",
    "        # Save DataFrame as CSV\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error converting JSON to CSV for {json_path}: {e}\"\n",
    "        print(error_message)\n",
    "        log_error(error_message)\n",
    "\n",
    "# Main function to process PDFs\n",
    "def process_pdfs(pdf_folder, output_folder):\n",
    "    # List all PDF files in the folder\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "\n",
    "        # Create output folders for each PDF\n",
    "        pdf_name = os.path.splitext(pdf_file)[0]\n",
    "        pdf_output_folder = os.path.join(output_folder, pdf_name)\n",
    "        os.makedirs(os.path.join(pdf_output_folder, 'extracted_json'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(pdf_output_folder, 'converted_csv'), exist_ok=True)\n",
    "\n",
    "        # Extract JSON and convert to CSV\n",
    "        json_path = extract_json_from_pdf(pdf_path, pdf_output_folder)\n",
    "        convert_json_to_csv(json_path, pdf_output_folder)\n",
    "\n",
    "# Specify the PDF folder and desired output location\n",
    "pdf_folder = \"./PDFs\"\n",
    "output_folder = \"./\"\n",
    "\n",
    "# Process the PDFs\n",
    "process_pdfs(pdf_folder, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
